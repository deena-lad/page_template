[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kirtan Gangani",
    "section": "",
    "text": "I am a recent B.Tech graduate in Computer Engineering from Ajeenkya DY Patil University, where I completed my degree with a GPA of 9.41/10. Currently, I’m a Research Intern at the Sustainability Lab, IIT Gandhinagar led by Prof. Nipun Batra, where I’m applying artificial intelligence to healthcare — specifically, building vision-based systems for sleep apnea detection using thermal imagery alone, without relying on wearable sensors.\n\n\nMy technical strengths lie in machine learning, computer vision, NLP, and large language models. I enjoy designing end-to-end ML pipelines and have worked extensively with frameworks like TensorFlow, Hugging Face Transformers, and YOLO architectures. I’m deeply motivated by research that blends practical impact with technical rigor, and I aspire to pursue a Ph.D. in Computer Science to further explore AI for real-world challenges.\n\n\nOutside of academics, I am a chess player with a peak Chess.com rapid rating of 1890."
  },
  {
    "objectID": "index.html#my-journey-so-far",
    "href": "index.html#my-journey-so-far",
    "title": "Kirtan Gangani",
    "section": "My Journey So Far",
    "text": "My Journey So Far\n\n\n\n\n\n\nMay 2025 – Present\n\n\nStarted working as Research Intern at Sustainability Lab, IIT Gandhinagar\n\n\n\n\nMay 2025\n\n\nCompleted B.Tech in Computer Engineering from Ajeenkya DY Patil University\n\n\n\n\nAug 2021\n\n\nStarted B.Tech in Computer Engineering at Ajeenkya DY Patil University"
  },
  {
    "objectID": "blog-posts/markov-chain/index.html",
    "href": "blog-posts/markov-chain/index.html",
    "title": "Understanding Markov Chains",
    "section": "",
    "text": "Markov chains, named after Andrey Markov is a powerful mathematical model used to show a sequence of events where the probability of each event depends only on the state achieved in the previous event. It doesn’t care about the entire history that led to the current state, just the current state itself. This unique “memoryless” property makes them surprisingly versatile and useful for modeling a wide range of real-world phenomena."
  },
  {
    "objectID": "blog-posts/markov-chain/index.html#states",
    "href": "blog-posts/markov-chain/index.html#states",
    "title": "Understanding Markov Chains",
    "section": "States",
    "text": "States\nStates are all the possible “situations” or “conditions” our system can be in. For example:\n\nWeather: Sunny, Cloudy, Rainy\n\nA Stock Price: Up, Down, Stagnant\n\nYour Mood: Happy, Neutral, Sad"
  },
  {
    "objectID": "blog-posts/markov-chain/index.html#transitions-and-probabilities",
    "href": "blog-posts/markov-chain/index.html#transitions-and-probabilities",
    "title": "Understanding Markov Chains",
    "section": "Transitions and Probabilities",
    "text": "Transitions and Probabilities\nA transition is simply the movement from one state to another. For example, the weather changing from “Sunny” to “Cloudy.”\nEach transition comes with a transition probability, which is the likelihood of moving from a current state to a next state. These probabilities are always between 0 and 1.\nNote: For any given state in a system, the sum of the probabilities of all possible transitions originating from that state (including the probability of transitioning back to itself) must always equal 1. This holds true regardless of the total number of states in the system."
  },
  {
    "objectID": "blog-posts/markov-chain/index.html#the-markov-property",
    "href": "blog-posts/markov-chain/index.html#the-markov-property",
    "title": "Understanding Markov Chains",
    "section": "The Markov Property",
    "text": "The Markov Property\nThe Markov Property means that the probability of moving to a future state depends only on the current state, and not on any of the states that came before it.\nImagine you’re playing Monopoly and you land on a specific square, it doesn’t matter if you got there by rolling 6 and 2, or by rolling 5 and 3. All that matters is that you are currently on that square. This “forgetfulness” of past events is what makes Markov Chains unique and, surprisingly, easier to model."
  },
  {
    "objectID": "blog-posts/brain-to-breath/index.html",
    "href": "blog-posts/brain-to-breath/index.html",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "",
    "text": "Did you ever wonder how brain and breathing is connected? Ever wondered how brain reacts when you are deeply focused, or when you are calm, or when you are excited? I’ve always been fascinated by the subtle ways our bodies react to different experiences. So, I decided to turn my own body into a living laboratory. Using a Vernier respiratory belt I have captured my breath across three distinct scenarios:\n1. The focused intensity of a chess match\n2. The serene calm of soft music\n3. The raw energy of metal music."
  },
  {
    "objectID": "blog-posts/brain-to-breath/index.html#study-1-the-focus-intensity-of-chess",
    "href": "blog-posts/brain-to-breath/index.html#study-1-the-focus-intensity-of-chess",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Study 1: The Focus-Intensity of Chess",
    "text": "Study 1: The Focus-Intensity of Chess\nFor this study, I played a 3-minute online chess game. The goal of playing chess was to observe how my breathing pattern changes under focus, quick-decision and in winning position.\nI avoided looking at the live data during the match to ensure my breathing patterns remained unconscious and natural. The results were truly shocking! (And yes, for the record: I won the match!)"
  },
  {
    "objectID": "blog-posts/brain-to-breath/index.html#study-2-ranges-of-music",
    "href": "blog-posts/brain-to-breath/index.html#study-2-ranges-of-music",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Study 2: Ranges of Music",
    "text": "Study 2: Ranges of Music\nFor this study, I played two very different genres of music to explore on emotional and physiological responses:\n\nLo-Fi Music: I played a regular lo-fi music. This genre is known for its calming and ambient qualities.\nGlam Rock: The reason behind chosing this genre was it is completely opposite to my first choice, this kind of musics are known for energetic, powerful vocals and rhythms.\n\nDuring both music sessions, I made an effort to keep my mind completely free of other thoughts, allowing the music to be the primary influence on my state."
  },
  {
    "objectID": "blog-posts/brain-to-breath/index.html#chess",
    "href": "blog-posts/brain-to-breath/index.html#chess",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Chess",
    "text": "Chess\nMy chess game offered wonderful insights into how our thought and pressure directly impacts respiration. The match naturally progressed through its three distinct stages: Opening, Middlegame, and Endgame.\nDuring the Opening, as a strong chess player, I felt confident and familiar with the initial moves. My breathing was normal and consistent.\nHowever, as the game transitioned into the Middlegame, the game required some critical decision-making, and my data clearly showed moments where I unconsciously held my breath. These natural pauses coincides precisely with periods of intense calculation.\nEven at the start of the Endgame, I was unconciously holding my breath as the position was not so easy. At last when I gained a winning advantage, my breathing gradually returned to normal. There were still some game decisive moments at last but ultimately I won.\n\n\n\nChess"
  },
  {
    "objectID": "blog-posts/brain-to-breath/index.html#music",
    "href": "blog-posts/brain-to-breath/index.html#music",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Music",
    "text": "Music\nI chose two tracks that perfectly contrasted each other to observe a range of physiological responses: Powfu’s “Death Bed (Coffee for Your Head)” (lo-fi) and Måneskin’s “Beggin’” (glam rock). The results, analyzed separately for each song, vividly illustrate how my breathing adapted to the energy and mood of the music.\n\nLo-Fi\nWith Powfu’s lo-fi track, the calm and ambient qualities of the music truly settled my breathing. My data showed a calmer, more consistent, and generally slower respiratory rate showing a state of relaxation. It was clear that the gentle rhythm of the song encouraged a feeling of peace.\n\n\n\nDeath Bed - By Powfu\n\n\n\n\nGlam Rock\nIn contrast, when Måneskin’s “Beggin’” started, my breathing shifted dramatically. This glam rock anthem, known for its powerful vocals, and high energy, immediately got me “vibing”. My respiratory patterns became more dramatic and responsive, with noticeable changes in breathing rate as the song kept on. This results overall showed how electrifying and energtic the song was.\n\n\n\nBeggin - By Maneskin"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog-posts/index.html",
    "href": "blog-posts/index.html",
    "title": "My Blogs",
    "section": "",
    "text": "How Does Brain Focus Influences Your Respiration?\n\n\n\nData Visualization\n\nMatplotlib\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Markov Chains\n\n\n\nProbability\n\nStatistics\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the LSTM Cell\n\n\n\nDeep Learning\n\nLSTM\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 10, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-posts/understanding-lstm/index.html",
    "href": "blog-posts/understanding-lstm/index.html",
    "title": "Understanding the LSTM Cell",
    "section": "",
    "text": "Imagine trying to understand a long conversation where you forget the beginning halfway through. Traditional Recurrent Neural Network (RNN) models often struggle with this “memory” problem when dealing with sequences of data like text, speech, or time series. As sequences get longer, the information from earlier steps gets “diluted” or “forgotten” or in other words, gradients become too small which is called gradient vanishing. This makes it hard for the RNN to learn long-term dependencies meaning that they can’t effectively connect information from far earlier parts of a sequence to make a current decision.\nThis is where Long Short-Term Memory (LSTM) networks comes in. LSTMs are a special type of RNNs designed to overcome this limitation, allowing AI to remember important information over long periods. They were introduced to mitigate gradient vanishing/exploding problem faced by standard RNNs.\nThis is achieved by a “cell state” that acts like a conveyor belt, carrying information through the network, allowing it to preserve information over long sequences. This is their “long-term memory.” LSTMs achieve their long-term memory capabilities through a unique internal structure called “gates.” These gates are like intelligent filters that control the flow of information in and out of the memory cell. More about gate will be explained in later sections."
  },
  {
    "objectID": "blog-posts/understanding-lstm/index.html#forget-gate",
    "href": "blog-posts/understanding-lstm/index.html#forget-gate",
    "title": "Understanding the LSTM Cell",
    "section": "Forget Gate",
    "text": "Forget Gate\nThe forget gate’s primary role is to decide what information from the previous cell state (c_{t-1}) should be discarded or “forgotten”. It takes the current input (x_t) and the hidden state from the last time step (h_{t-1}) as inputs. These are passed through a sigmoid activation function.\n\\[\\begin{aligned}\nf_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n\\end{aligned}\\]\nThe output f_t is a vector with values from 0 to 1. This output is then element-wise multiplied with the previous cell state (c_{t-1}). A value of 1 means “keep all of this information”, while a value of 0 means “forget all of this informtion”. A value of 0.5 would indicate keep half of that information."
  },
  {
    "objectID": "blog-posts/understanding-lstm/index.html#input-gate",
    "href": "blog-posts/understanding-lstm/index.html#input-gate",
    "title": "Understanding the LSTM Cell",
    "section": "Input Gate:",
    "text": "Input Gate:\nThe input gate is responsible for determining which new information from the current input should be stored in the cell state. It works in two parts:\n\nCandidate Memory (_t)\nBefore the input gate itself, there’s the candidate memory, often denoted as _t. Its purpose is to propose new information that could be added to the cell state. Unlike the gates, which use sigmoid, the candidate memory uses a hyperbolic tangent (tanh) activation function. The tanh function outputs values between -1 and 1, allowing for both positive and negative contributions to the cell state.\n\\[\\begin{aligned}\n\\tilde{c}_t &= \\tanh(W_c [h_{t-1}, x_t] + b_c)\n\\end{aligned}\\]\n\n\nInput gate (i_t)\nThe input gate then decides how much of this newly proposed candidate memory t should actually be added to the cell state. Similar to the forget gate, it takes the current input (x_t) and the previous hidden state (h{t-1})and passes them through a sigmoid activation function.\n\\[\\begin{aligned}\ni_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n\\end{aligned}\\]\nthe output i_t acts as a filter for the candidate memory _t\n\n\nCell state Update (\\(c_t\\))\nThis is where the magic happens for updating the long-term memory of the network. The new cell state (\\(c_t\\)) is combination of two components:\n1. The information from the previous cell state (\\(c_{t−1}\\)) that the forget gate (\\(f_t\\)) decided to keep.\n2. The new candidate memory (\\(\\tilde{c}_t\\)) that the input gate (\\(i_t\\)) decided to add.\n\\[\\begin{aligned}\nc_t &= f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t\n\\end{aligned}\\]\nThese two parts are element-wise added to form the updared cell state (\\(c_t\\)).\nThis updated cell state \\(c_t\\) then carries the network’s long-term memory forward to the next time step."
  },
  {
    "objectID": "blog-posts/understanding-lstm/index.html#output-gate",
    "href": "blog-posts/understanding-lstm/index.html#output-gate",
    "title": "Understanding the LSTM Cell",
    "section": "Output Gate",
    "text": "Output Gate\nFinally, the output gate controls how much of the updated cell state (\\(c_t\\)) will be exposed as the current hidden state (\\(h_t\\)). The hidden state is the output of the LSTM cell at the current time step and is also passed on to the next time step.\nFirst, the output gate determines which parts of the cell state are relevant for the current hidden state. It uses the current input (\\(x_t\\)) and the previous hidden state (\\(h_{t−1}\\)) passed through a sigmoid function:\n\\[\\begin{aligned}\no_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n\\end{aligned}\\]\nNext, the updated cell state (\\(c_t\\)) is passed through a tanh activation function. This scales the cell state values to between -1 and 1, making them ready to be filtered.\nFinally, the result of the tanh operation on \\(c_t\\) is element-wise multiplied by the output gate’s activation (\\(o_t\\)) to produce the new hidden state (\\(h_t\\)):\n\\[\\begin{aligned}\nh_t &= o_t \\cdot \\tanh(c_t)\n\\end{aligned}\\]\nThe hidden state \\(h_t\\) serves as the output of the LSTM block for the current time step and is also used as an input to the gates in the next time step."
  },
  {
    "objectID": "blog-posts/markov-chain/index.html#discrete-time-vs.-continuous-time",
    "href": "blog-posts/markov-chain/index.html#discrete-time-vs.-continuous-time",
    "title": "Understanding Markov Chains",
    "section": "Discrete-Time vs. Continuous-Time",
    "text": "Discrete-Time vs. Continuous-Time\n\nDiscrete-Time Markov Chains (DTMC): In a DTMC, transitions between states occur at fixed, regular intervals or “steps.” Think of it like taking a snapshot of the system every hour, day, or year. Our weather example is a perfect DTMC because we’re looking at the weather “tomorrow” based on “today.” The transition matrix you’ve seen applies directly to DTMCs.\nContinuous-Time Markov Chains (CTMC): In contrast, CTMCs allow transitions to occur at any point in time, not just at predefined intervals. The amount of time spent in a particular state before transitioning is a random variable, often modeled by an exponential distribution. While more complex mathematically, CTMCs are used to model systems where events happen asynchronously, like customer arrivals in a queue or radioactive decay."
  },
  {
    "objectID": "blog-posts/markov-chain/index.html#discrete-state-vs.-continuous-state",
    "href": "blog-posts/markov-chain/index.html#discrete-state-vs.-continuous-state",
    "title": "Understanding Markov Chains",
    "section": "Discrete-State vs. Continuous-State",
    "text": "Discrete-State vs. Continuous-State\nThis distinction refers to the nature of the states themselves.\n\nDiscrete-State Markov Chains: This is what we’ve been discussing. The system can only be in a finite (or countably infinite) number of distinct, separate states. Examples include: “Sunny,” “Cloudy,” “Rainy”; “Up,” “Down,” “Stagnant” for stock prices; or even the squares on a Monopoly board. All the examples and the transition matrix we’ve used so far fall into this category.\nContinuous-State Markov Chains: In these chains, the state space is continuous, meaning the system can take on any value within a range. For instance, modeling the exact temperature (e.g., 25.3 degrees, 25.31 degrees, etc.) or the precise stock price value over time. These are often more complex and are typically modeled using stochastic differential equations, moving beyond simple transition matrices. For an introductory understanding, focusing on discrete-state chains is appropriate."
  },
  {
    "objectID": "blog-posts/understanding-lstm/index.html#advantages-of-lstms",
    "href": "blog-posts/understanding-lstm/index.html#advantages-of-lstms",
    "title": "Understanding the LSTM Cell",
    "section": "Advantages of LSTMs:",
    "text": "Advantages of LSTMs:\n\nSolving the Vanishing Gradient Problem: This is their most significant advantage. LSTMs effectively address the vanishing gradient problem inherent in traditional RNNs, allowing them to learn and retain information over very long sequences. This is primarily due to their unique cell state and gate mechanisms that regulate information flow.\nCapturing Long-Term Dependencies: Thanks to their ability to maintain a persistent cell state, LSTMs can connect information from distant past steps to make decisions in the present. This is crucial for tasks like understanding context in long sentences or predicting future values in time series based on historical trends.\nHandling Variable-Length Sequences: LSTMs are naturally designed to process sequences of varying lengths, making them highly versatile for real-world data like text (sentences of different lengths), speech (utterances of different durations), and time series.\nRobustness to Noise (to some extent): The gating mechanism allows LSTMs to selectively filter out irrelevant or noisy information, focusing on the most important features in the sequence.\nWide Applicability: LSTMs have found immense success across a broad spectrum of domains, including:\n\nNatural Language Processing (NLP): Machine translation, sentiment analysis, text summarization, named entity recognition, language modeling.\nSpeech Recognition: Converting spoken words into text.\nTime Series Forecasting: Predicting stock prices, weather patterns, energy consumption.\nVideo Processing: Action recognition, video captioning."
  },
  {
    "objectID": "blog-posts/understanding-lstm/index.html#disadvantages-of-lstms",
    "href": "blog-posts/understanding-lstm/index.html#disadvantages-of-lstms",
    "title": "Understanding the LSTM Cell",
    "section": "Disadvantages of LSTMs:",
    "text": "Disadvantages of LSTMs:\n\nComputational Cost: LSTMs are more computationally intensive and slower to train compared to simpler neural networks or even standard RNNs. This is due to the increased number of parameters (weights and biases for each gate) and the complex calculations involved in their internal mechanisms.\nComplex Architecture: The multiple gates and intricate interactions within an LSTM cell make them more complex to understand and debug compared to simpler models. While powerful, this complexity can be a barrier for newcomers.\nDifficulty with Very Long Sequences: Although LSTMs significantly mitigate the long-term dependency problem, they can still struggle with extremely long sequences. As the sequence length increases, the computational burden grows, and even LSTMs can start to lose efficiency or effectiveness.\nLimited Parallelization: The inherent sequential nature of LSTMs (processing one time step after another) makes it challenging to fully parallelize their training across multiple computing cores or GPUs, especially during the forward and backward passes within a sequence. This is a key area where newer architectures like Transformers have shown significant advantages.\nHyperparameter Tuning: LSTMs often require careful tuning of hyperparameters (e.g., number of hidden units, learning rate, dropout) to achieve optimal performance, which can be a time-consuming process.\nOutshined by Transformers in Many NLP Tasks: For many state-of-the-art NLP tasks, transformer architectures (which use attention mechanisms instead of recurrence) have largely surpassed LSTMs in performance, particularly for very long sequences and tasks requiring complex contextual understanding."
  }
]