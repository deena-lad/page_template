[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kirtan Gangani",
    "section": "",
    "text": "GitHub\n      \n      \n        \n          LinkedIn\n      \n      \n        \n          Email\n      \n    \n  \n  \n    \n      I am a recent B.Tech graduate in Computer Engineering from Ajeenkya DY Patil University, where I completed my degree with a GPA of 9.41/10. Currently, I’m a Research Intern at the Sustainability Lab, IIT Gandhinagar led by Prof. Nipun Batra, where I’m applying artificial intelligence to healthcare — specifically, building vision-based systems for sleep apnea detection using thermal imagery alone, without relying on wearable sensors.\n    \n\n    \n      My technical strengths lie in machine learning, computer vision, NLP, and large language models. I enjoy designing end-to-end ML pipelines and have worked extensively with frameworks like TensorFlow, Hugging Face Transformers, and YOLO architectures. I’m deeply motivated by research that blends practical impact with technical rigor, and I aspire to pursue a Ph.D. in Computer Science to further explore AI for real-world challenges.\n    \n\n    \n      Outside of academics, I am a chess player with a peak Chess.com rapid rating of 1890."
  },
  {
    "objectID": "index.html#my-journey-so-far",
    "href": "index.html#my-journey-so-far",
    "title": "Kirtan Gangani",
    "section": "My Journey So Far",
    "text": "My Journey So Far\n\n\n\n9 May 2025 – 15 July 2025\n\n\nSuccessfully Completed Research Internship at Sustainability Lab, IIT Gandhinagar\n\n\n\n\nMay 2025\n\n\nCompleted B.Tech in Computer Engineering from Ajeenkya DY Patil University\n\n\n\n\nAug 2021\n\n\nStarted B.Tech in Computer Engineering at Ajeenkya DY Patil University"
  },
  {
    "objectID": "blog-posts/sampling-using-uniform-distribution.html",
    "href": "blog-posts/sampling-using-uniform-distribution.html",
    "title": "Sampling From Categorical Distribution Using Uniform Distribution",
    "section": "",
    "text": "Have you ever wondered how computer programs make “random” choices with specific probabilities? Whether it’s deciding the weather in a simulation or picking a random item from a loot table in a game, the underlying mechanism often involves sampling from a categorical distribution.\nA categorical distribution simply defines the probability of selecting each item from a finite set of categories. For example, if we were predicting the weather today in Gandhinagar, Gujarat, India, we might have the following (simplified) probabilities:\n\nSunny: 65%\nRainy: 35%\nCloudy: 10%\n\nSo, how do we get a computer to make a “random” choice that respects these probabilities? The answer lies in the power of the uniform distribution."
  },
  {
    "objectID": "blog-posts/sampling-using-uniform-distribution.html#categorical-distribution",
    "href": "blog-posts/sampling-using-uniform-distribution.html#categorical-distribution",
    "title": "Sampling From Categorical Distribution Using Uniform Distribution",
    "section": "Categorical distribution",
    "text": "Categorical distribution\nThis describes the probabilities of a discrete random variable taking on one of a fixed set of categories (e.g., “Sunny”, “Cloudy”, “Rainy”). Each category has a specific probability, and these probabilities must sum to 1.\nExample:\n\nSunny: 0.65 probability\nCloudy: 0.25 probabilty\nRainy: 0.1 probability"
  },
  {
    "objectID": "blog-posts/sampling-using-uniform-distribution.html#uniform-distributions",
    "href": "blog-posts/sampling-using-uniform-distribution.html#uniform-distributions",
    "title": "Sampling From Categorical Distribution Using Uniform Distribution",
    "section": "Uniform distributions",
    "text": "Uniform distributions\nThis is typically a continuous uniform distribution over the interval [0,1]. It means that any value between 0 and 1 is equally likely to be drawn."
  },
  {
    "objectID": "blog-posts/sampling-using-uniform-distribution.html#creating-the-cumulative-probability",
    "href": "blog-posts/sampling-using-uniform-distribution.html#creating-the-cumulative-probability",
    "title": "Sampling From Categorical Distribution Using Uniform Distribution",
    "section": "Creating the cumulative probability",
    "text": "Creating the cumulative probability\nThis is simply the running total of the probabilities. For our Gandhinagar weather example:\n\nSunny: 65% (Cumulative: 65%) -&gt; Interval: [0.00, 0.65)\nRainy: 25% (Cumulative: 65% + 25% = 90%) -&gt; Interval: [0.65, 0.90)\nCloudy: 10% (Cumulative: 90% + 10% = 100%) -&gt; Interval: [0.90, 1.00]"
  },
  {
    "objectID": "blog-posts/sampling-using-uniform-distribution.html#sampling-process",
    "href": "blog-posts/sampling-using-uniform-distribution.html#sampling-process",
    "title": "Sampling From Categorical Distribution Using Uniform Distribution",
    "section": "Sampling process",
    "text": "Sampling process\nNow, we generate a single random number between 0 and 1 from our uniform distribution. The interval in which this random number falls directly corresponds to the category we select.\n\nIf our random number is between 0.00 and 0.65 (exclusive of 0.65), we choose “Sunny.”\nIf it’s between 0.65 and 0.90 (exclusive of 0.90), we choose “Rainy.”\nIf it’s between 0.90 and 1.00 (inclusive), we choose “Cloudy.”\n\nSince every number between 0 and 1 has an equal chance of being generated, the likelihood of our random number landing in a particular interval is directly proportional to the size (probability) of that interval."
  },
  {
    "objectID": "blog-posts/minima-maxima.html",
    "href": "blog-posts/minima-maxima.html",
    "title": "Finding Minimas and Maximas using SciPy",
    "section": "",
    "text": "Identifying maxima and minima is a fundamental task if you are analyzing sensor readings, optimizing a machine learning model, or studying scientific phenomena. Fortunately, Python’s SciPy library provides an efficient way to do that.\nIn this blog post, we’ll dive into how to use SciPy for finding local extrema (both maximas and minimas) in respiratory force data. We will also explore how to achieve similar results with a manual approach, giving us deeper understanding of the underlying logic."
  },
  {
    "objectID": "blog-posts/minima-maxima.html#maximas-peaks",
    "href": "blog-posts/minima-maxima.html#maximas-peaks",
    "title": "Finding Minimas and Maximas using SciPy",
    "section": "Maximas (Peaks)",
    "text": "Maximas (Peaks)\nThink of these as the “high points” or “peaks” in your data. In our Force data, a maxima would be a moment when the force measurement reaches a peak value, and then starts to decrease.\n\nLocal Maxima: These are peaks that are higher than the immediately surrounding data points. This means a point is considered a maxima if it is strictly greater to its left and right neighbor.\nGlobal Maximum: This would be the absolute highest value recorded across your entire dataset. While find_peaks identifies local peaks, you’d need an extra step (like np.max(force)) to find the global maximum."
  },
  {
    "objectID": "blog-posts/minima-maxima.html#minima-valleys",
    "href": "blog-posts/minima-maxima.html#minima-valleys",
    "title": "Finding Minimas and Maximas using SciPy",
    "section": "Minima (Valleys)",
    "text": "Minima (Valleys)\nMinima are the “low points”. In our Force data, a minima would be when the force measurement dips to a low value and then starts to increase. This means a point is considered a minima if it is strictly lesser to its left and right neighbor.\n\nLocal Minima: These are valleys that are lower than the immediately surrounding data points.\nGlobal Minimum: This would be the absolute lowest value recorded across your entire dataset. Similar to the global maximum, find_peaks (when applied to -force for valleys) identifies local minima, and you’d use np.min(force) for the global minimum."
  },
  {
    "objectID": "blog-posts/index.html",
    "href": "blog-posts/index.html",
    "title": "My Blogs",
    "section": "",
    "text": "Don’t Let These Confusing English Words Trip You Up!\n\n\n\nEnglish\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Fast Fourier Transformer (FFT)\n\n\n\nSignal Processing\n\nFFT\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore GitHub Beyond Codes\n\n\n\ngit\n\nportfolio\n\npersonal branding\n\nweb development\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nFinding Minimas and Maximas using SciPy\n\n\n\nPython\n\nSciPy\n\nData Visualisation\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPseudoRandom Number Generator (PRNG)\n\n\n\nDeterministic\n\nSeed-based\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling From Categorical Distribution Using Uniform Distribution\n\n\n\nStatistics\n\nProbability\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding ConvLSTM\n\n\n\nCNN\n\nLSTM\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nHow Does Brain Focus Influences Your Respiration?\n\n\n\nData Visualization\n\nMatplotlib\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Markov Chains\n\n\n\nProbability\n\nStatistics\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the LSTM Cell\n\n\n\nDeep Learning\n\nLSTM\n\n\n\n\n\n\n\nKirtan Gangani\n\n\nJul 10, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-posts/fast-fourier-transformer.html",
    "href": "blog-posts/fast-fourier-transformer.html",
    "title": "Introduction to Fast Fourier Transformer (FFT)",
    "section": "",
    "text": "Ever wondered how your phone recognizes your voice, how medical images are reconstructed, or how noise is filtered out of audio? Often working silently behind these everyday marvels is a powerful mathematical tool that might initially seem intimidating: the Fast Fourier Transform (FFT). It’s fundamental to signal processing, image processing, data analysis, and so much more. But what exactly is it, and why is it so ‘fast’?\n\n\nImagine you have a complex sound, say, a mix of a flute, a drum, and a violin playing simultaneously. If you look at the sound wave over time, it just looks like a wobbly line. It’s hard to tell what instruments are playing just by looking at that line.\nThis is where the Fourier Transform comes in. It is a mathematical operation that takes a signal (like our sound wave) from the time domain to the frequency domain. In the frequency domain, instead of seeing how the signal changes over time, you see what frequencies are present in that signal and how strong each frequency is.\nThink of it like this:\n\nTime Domain: You hear the whole song at once.\nFrequency Domain: You can see individual musical notes (frequencies) and how loud each one is, allowing you to identify the flute’s high notes, the drum’s low thumps, and the violin’s mid-range melodies.\n\n\n\n\nThe basic Fourier Transform (specifically, the Discrete Fourier Transform or DFT for discrete digital signals) is computationally intensive. If you have N data points, calculating the DFT takes about \\(N^2\\) operations. For large datasets, this can be incredibly slow.\nThis is why we need Fast Fourier Transform (FFT)! It’s not a different transform, but rather an efficient algorithm for computing the DFT. It drastically reduces the number of computations, particularly when N is a power of 2. Instead of \\(N^2\\), the FFT takes roughly \\(N \\log_2(N)\\) operations.\nTo put that into perspective:\n\nIf \\(N = 1024\\) (a common power of 2):\n\nDFT operations: \\(1024^2 \\approx 1,000,000\\)\nFFT operations: \\(1024 \\times \\log_2(1024) = 1024 \\times 10 \\approx 10,000\\) That’s a 100-fold speedup! This efficiency is what made the Fourier Transform practical for real-world applications.\n\n\nThe ‘trick’ behind the FFT often involves a divide-and-conquer approach, breaking down the large DFT calculation into smaller DFTs, and then cleverly combining the results. The most famous FFT algorithm is the Cooley-Tukey algorithm.\n\n\n\nThe output of the FFT is an array of complex numbers. Each complex number represents a specific frequency component in the original signal.\nA complex number has two parts: a real part and an imaginary part. From these, we can derive two important pieces of information for each frequency:\n\nMagnitude (Amplitude): This tells us how strong or prominent a particular frequency component is in the original signal. A larger magnitude means that frequency contributes more to the signal.\n\nCalculated as: \\(\\sqrt{(\\text{real part})^2 + (\\text{imaginary part})^2}\\)\n\nPhase: This tells us about the shift or delay of that frequency component relative to a reference. While often less intuitive for beginners, phase is crucial in applications like signal reconstruction.\n\nThe output array is also symmetric. The first half of the array typically contains the positive frequencies, and the second half (or wrapped around to the beginning for specific conventions) contains the negative frequencies. For real-valued input signals, the negative frequencies are just mirror images of the positive ones, so we often focus on the first half of the spectrum.\n\n\n\nThe FFT’s versatility makes it highly recommended across many fields:\n\nSignal Processing: Audio compression (MP3, AAC), noise reduction, speech recognition, telecommunications.\nImage Processing: Image compression (JPEG), edge detection, filtering, medical imaging (MRI, CT scans).\nData Analysis: Financial market analysis, analyzing vibrations in mechanical systems, earthquake seismology.\nPhysics & Engineering: Quantum mechanics, optics, structural analysis, circuit design.\nAstronomy: Analyzing signals from space, processing telescope data.\n\n\n\n\nLet’s see the FFT in action! Python’s NumPy library provides a highly optimized FFT implementation. We’ll generate a simple signal consisting of two sine waves and then use FFT to reveal their frequencies.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nFs = 1000            # Sampling frequency\nL = 1500             # Number of samples\nt = np.arange(L) / Fs  # Time vector\n\nf1, f2 = 50, 120\nsignal = 0.7 * np.sin(2 * np.pi * f1 * t) + 1.5 * np.sin(2 * np.pi * f2 * t) + 0.2 * np.random.randn(L)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(2, 1, 1)\nplt.plot(t, signal)\nplt.title('Time Domain Signal')\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.grid(True)\n\nY = np.fft.fft(signal)\nP = np.abs(Y / L)[:L//2+1]\nP[1:-1] *= 2\nf = np.linspace(0, Fs/2, L//2 + 1)\n\nplt.subplot(2, 1, 2)\nplt.plot(f, P)\nplt.title('Single-Sided Amplitude Spectrum')\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('Amplitude')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\ndominant_freqs = f[np.argsort(P[1:])[-2:] + 1]\nprint(f\"Dominant frequencies: {dominant_freqs[1]:.2f} Hz and {dominant_freqs[0]:.2f} Hz\")\n\n\n\n\n\n\n\n\nDominant frequencies: 120.00 Hz and 50.00 Hz\n\n\nIn the plot above, you can clearly see two prominent peaks in the frequency domain, corresponding to our 50 Hz and 120 Hz sine waves, even with some added noise! This demonstrates the power of FFT in identifying the constituent frequencies of a signal."
  },
  {
    "objectID": "blog-posts/fast-fourier-transformer.html#the-core-idea",
    "href": "blog-posts/fast-fourier-transformer.html#the-core-idea",
    "title": "Introduction to Fast Fourier Transformer (FFT)",
    "section": "",
    "text": "Imagine you have a complex sound, say, a mix of a flute, a drum, and a violin playing simultaneously. If you look at the sound wave over time, it just looks like a wobbly line. It’s hard to tell what instruments are playing just by looking at that line.\nThis is where the Fourier Transform comes in. It is a mathematical operation that takes a signal (like our sound wave) from the time domain to the frequency domain. In the frequency domain, instead of seeing how the signal changes over time, you see what frequencies are present in that signal and how strong each frequency is.\nThink of it like this:\n\nTime Domain: You hear the whole song at once.\nFrequency Domain: You can see individual musical notes (frequencies) and how loud each one is, allowing you to identify the flute’s high notes, the drum’s low thumps, and the violin’s mid-range melodies."
  },
  {
    "objectID": "blog-posts/fast-fourier-transformer.html#why-fast",
    "href": "blog-posts/fast-fourier-transformer.html#why-fast",
    "title": "Introduction to Fast Fourier Transformer (FFT)",
    "section": "",
    "text": "The basic Fourier Transform (specifically, the Discrete Fourier Transform or DFT for discrete digital signals) is computationally intensive. If you have N data points, calculating the DFT takes about \\(N^2\\) operations. For large datasets, this can be incredibly slow.\nThis is why we need Fast Fourier Transform (FFT)! It’s not a different transform, but rather an efficient algorithm for computing the DFT. It drastically reduces the number of computations, particularly when N is a power of 2. Instead of \\(N^2\\), the FFT takes roughly \\(N \\log_2(N)\\) operations.\nTo put that into perspective:\n\nIf \\(N = 1024\\) (a common power of 2):\n\nDFT operations: \\(1024^2 \\approx 1,000,000\\)\nFFT operations: \\(1024 \\times \\log_2(1024) = 1024 \\times 10 \\approx 10,000\\) That’s a 100-fold speedup! This efficiency is what made the Fourier Transform practical for real-world applications.\n\n\nThe ‘trick’ behind the FFT often involves a divide-and-conquer approach, breaking down the large DFT calculation into smaller DFTs, and then cleverly combining the results. The most famous FFT algorithm is the Cooley-Tukey algorithm."
  },
  {
    "objectID": "blog-posts/fast-fourier-transformer.html#what-does-the-fft-output-look-like",
    "href": "blog-posts/fast-fourier-transformer.html#what-does-the-fft-output-look-like",
    "title": "Introduction to Fast Fourier Transformer (FFT)",
    "section": "",
    "text": "The output of the FFT is an array of complex numbers. Each complex number represents a specific frequency component in the original signal.\nA complex number has two parts: a real part and an imaginary part. From these, we can derive two important pieces of information for each frequency:\n\nMagnitude (Amplitude): This tells us how strong or prominent a particular frequency component is in the original signal. A larger magnitude means that frequency contributes more to the signal.\n\nCalculated as: \\(\\sqrt{(\\text{real part})^2 + (\\text{imaginary part})^2}\\)\n\nPhase: This tells us about the shift or delay of that frequency component relative to a reference. While often less intuitive for beginners, phase is crucial in applications like signal reconstruction.\n\nThe output array is also symmetric. The first half of the array typically contains the positive frequencies, and the second half (or wrapped around to the beginning for specific conventions) contains the negative frequencies. For real-valued input signals, the negative frequencies are just mirror images of the positive ones, so we often focus on the first half of the spectrum."
  },
  {
    "objectID": "blog-posts/fast-fourier-transformer.html#applications-of-fft-beyond-sound",
    "href": "blog-posts/fast-fourier-transformer.html#applications-of-fft-beyond-sound",
    "title": "Introduction to Fast Fourier Transformer (FFT)",
    "section": "",
    "text": "The FFT’s versatility makes it highly recommended across many fields:\n\nSignal Processing: Audio compression (MP3, AAC), noise reduction, speech recognition, telecommunications.\nImage Processing: Image compression (JPEG), edge detection, filtering, medical imaging (MRI, CT scans).\nData Analysis: Financial market analysis, analyzing vibrations in mechanical systems, earthquake seismology.\nPhysics & Engineering: Quantum mechanics, optics, structural analysis, circuit design.\nAstronomy: Analyzing signals from space, processing telescope data."
  },
  {
    "objectID": "blog-posts/fast-fourier-transformer.html#a-simple-code-implementation-in-python",
    "href": "blog-posts/fast-fourier-transformer.html#a-simple-code-implementation-in-python",
    "title": "Introduction to Fast Fourier Transformer (FFT)",
    "section": "",
    "text": "Let’s see the FFT in action! Python’s NumPy library provides a highly optimized FFT implementation. We’ll generate a simple signal consisting of two sine waves and then use FFT to reveal their frequencies.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nFs = 1000            # Sampling frequency\nL = 1500             # Number of samples\nt = np.arange(L) / Fs  # Time vector\n\nf1, f2 = 50, 120\nsignal = 0.7 * np.sin(2 * np.pi * f1 * t) + 1.5 * np.sin(2 * np.pi * f2 * t) + 0.2 * np.random.randn(L)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(2, 1, 1)\nplt.plot(t, signal)\nplt.title('Time Domain Signal')\nplt.xlabel('Time [s]')\nplt.ylabel('Amplitude')\nplt.grid(True)\n\nY = np.fft.fft(signal)\nP = np.abs(Y / L)[:L//2+1]\nP[1:-1] *= 2\nf = np.linspace(0, Fs/2, L//2 + 1)\n\nplt.subplot(2, 1, 2)\nplt.plot(f, P)\nplt.title('Single-Sided Amplitude Spectrum')\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('Amplitude')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\ndominant_freqs = f[np.argsort(P[1:])[-2:] + 1]\nprint(f\"Dominant frequencies: {dominant_freqs[1]:.2f} Hz and {dominant_freqs[0]:.2f} Hz\")\n\n\n\n\n\n\n\n\nDominant frequencies: 120.00 Hz and 50.00 Hz\n\n\nIn the plot above, you can clearly see two prominent peaks in the frequency domain, corresponding to our 50 Hz and 120 Hz sine waves, even with some added noise! This demonstrates the power of FFT in identifying the constituent frequencies of a signal."
  },
  {
    "objectID": "blog-posts/confusing-english.html",
    "href": "blog-posts/confusing-english.html",
    "title": "Don’t Let These Confusing English Words Trip You Up!",
    "section": "",
    "text": "English can be a tricky language, full of nuances, exceptions, and words that sound similar but have completely different meanings. Even native speakers can stumble over common confusions. This blog is to clear confusions for some of the most common word pairs and help you speak and write with greater confidence.\n\n1. It’s vs. its: understand the apostrophe\nIt’s: This is a contraction of “it is” or “it has.”\n\nExample: It’s a beautiful day outside. (It is a beautiful day.)\nExample: It’s been a long time since we last met. (It has been a long time.)\n\nIts: This is a possessive pronoun, showing ownership (like “his” or “hers”).\n\nExample: The dog wagged its tail happily. (The tail belongs to the dog.)\nExample: The company is proud of its achievements.\n\n\n\n2. Me vs. I\nThis often trips people up when they’re talking about themselves in conjunction with others. The key is understanding subjects and objects.\nI: This is a subject pronoun. It performs the action in a sentence.\n\nExample: I went to the store.\nExample: John and I are going to the concert.\n\nMe: This is an object pronoun. It receives the action or is the object of a preposition.\n\nExample: Sarah gave the book to me.\nExample: The dog followed John and me home.\n\nTip: To check, remove the other person from the sentence. Would you say “Me went to the store”? No. Would you say “Sarah gave the book to I”? No.\n\n\n3. That vs. Which\nThis can feel a bit confusing, but once you get it, it’s straightforward. It’s all about whether the information is essential.\nThat: Used for restrictive clauses (essential information). The clause beginning with “that” provides information that is crucial to the meaning of the sentence. Without it, the sentence’s meaning would change significantly. No comma is used before “that.”\n\nExample: This is the car that I want to buy. (The clause “that I want to buy” specifies which car it is.)\n\nWhich: Used for non-restrictive clauses (non-essential or extra information). The clause beginning with “which” provides additional information that isn’t crucial to the sentence’s main meaning. These clauses are set off by commas.\n\nExample: The old car, which has a rusted bumper, still runs perfectly. (The fact that it has a rusted bumper is extra info; the car still runs perfectly whether it has one or not.)\n\nTip: If you can remove the clause without changing the fundamental meaning of the sentence, use “which” and surround it with commas. If you can’t, use “that.”\n\n\n4. Quite vs. Quiet\nThese two sound very similar but have distinct meanings.\nQuite: This is an adverb meaning “to a considerable extent,” “very,” or “completely.”\n\nExample: I’m feeling quite tired after the long journey. (Considerably tired)\nExample: She was quite sure of her answer. (Completely sure)\n\nQuiet: This is an adjective meaning “making little or no noise,” or “calm.”\n\nExample: Please be quiet in the library. (Make no noise)\nExample: The lake was calm and quiet this morning. (Peaceful and still)\n\nTip: Think of “quiet” as relating to sound or lack thereof. “Quite” is about degree or intensity.\n\n\n5. Affect vs. Effect\nThis is a classic!\nAffect (verb): To influence or produce a change in something.\n\nExample: The bad weather will affect our travel plans.\nExample: His words deeply affected her.\n\nEffect (noun): The result or consequence of an action.\nExample: The new policy had a positive effect on the economy.\nExample: What was the effect of the medicine?\n\n\n6. Then vs. Than\nAnother pair that sounds alike but has different functions.\nThen (adverb): Refers to time or sequence.\n\nExample: We went to the park, and then we had ice cream.\nExample: If you finish your homework, then you can watch TV.\n\nThan (conjunction/preposition): Used for comparisons.\nExample: She is taller than her brother.\nExample: I prefer coffee more than tea.\n\n\n7. Lose vs. Loose\nA very common spelling and usage error.\nLose (verb): To misplace something, to fail to win, or to be deprived of.\n\nExample: Don’t lose your keys!\nExample: Our team might lose the game.\nExample: He started to lose his patience.\n\nLoose (adjective): Not tight or confined; free from restraint.\n\nExample: My shoelaces are loose.\nExample: The dog was running loose in the park.\n\nTip: “Loose” rhymes with “goose” (and often describes something that is “goose-like” in its freedom!). “Lose” rhymes with “choose.”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog-posts/brain-to-breath.html",
    "href": "blog-posts/brain-to-breath.html",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "",
    "text": "Did you ever wonder how brain and breathing is connected? Ever wondered how brain reacts when you are deeply focused, or when you are calm, or when you are excited? I’ve always been fascinated by the subtle ways our bodies react to different experiences. So, I decided to turn my own body into a living laboratory. Using a Vernier respiratory belt I have captured my breath across three distinct scenarios:\n1. The focused intensity of a chess match\n2. The serene calm of soft music\n3. The raw energy of metal music."
  },
  {
    "objectID": "blog-posts/brain-to-breath.html#study-1-the-focus-intensity-of-chess",
    "href": "blog-posts/brain-to-breath.html#study-1-the-focus-intensity-of-chess",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Study 1: The Focus-Intensity of Chess",
    "text": "Study 1: The Focus-Intensity of Chess\nFor this study, I played a 3-minute online chess game. The goal of playing chess was to observe how my breathing pattern changes under focus, quick-decision and in winning position.\nI avoided looking at the live data during the match to ensure my breathing patterns remained unconscious and natural. The results were truly shocking! (And yes, for the record: I won the match!)"
  },
  {
    "objectID": "blog-posts/brain-to-breath.html#study-2-ranges-of-music",
    "href": "blog-posts/brain-to-breath.html#study-2-ranges-of-music",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Study 2: Ranges of Music",
    "text": "Study 2: Ranges of Music\nFor this study, I played two very different genres of music to explore on emotional and physiological responses:\n\nLo-Fi Music: I played a regular lo-fi music. This genre is known for its calming and ambient qualities.\nGlam Rock: The reason behind chosing this genre was it is completely opposite to my first choice, this kind of musics are known for energetic, powerful vocals and rhythms.\n\nDuring both music sessions, I made an effort to keep my mind completely free of other thoughts, allowing the music to be the primary influence on my state."
  },
  {
    "objectID": "blog-posts/brain-to-breath.html#chess",
    "href": "blog-posts/brain-to-breath.html#chess",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Chess",
    "text": "Chess\nMy chess game offered wonderful insights into how our thought and pressure directly impacts respiration. My overall respiration rate (BPM) averaged approximately 9.52, ranging from a minimum of 6.44 BPM to a maximum of 11.58 BPM. The average respiratory force (amplitude) was about 17.01 N. The match naturally progressed through its three distinct stages: Opening, Middlegame, and Endgame.\nDuring the Opening, as a strong chess player, I felt confident and familiar with the initial moves. My breathing was normal and consistent.\nHowever, as the game transitioned into the Middlegame, the game required some critical decision-making, and my data clearly showed moments where I unconsciously held my breath – periods shown by noticeable dips or plateaus in my breathing rate, sometimes approaching the minimum of 6.44 BPM. These natural pauses coincides precisely with periods of intense calculation. The highest recorded force, a peak of 36.42 N, likely indicates more forceful, compensatory breaths taken after these periods of breath-holding, as my body worked to regain oxygen balance.\nEven at the start of the Endgame, I was unconciously holding my breath as the position was not so easy. At last when I gained a winning advantage, my breathing gradually returned to normal. There were still some game decisive moments at last but ultimately I won.\n\n\n\nFigure 1. This graph shows the “force” or amplitude of my chest movements during the game. Notice the sustained periods of lower force and more rough patterns during the intense calculation phases, interspersed with sharp increases as my body compensated.\n\n\n\nFigure 2. This graph displays my breathing rate in Breaths Per Minute (BPM). The slight dips and less consistent pattern during the intense phases of the game correspond to moments where my breathing rate decreased due to unconscious breath-holding, sometimes reaching a minimum of 6.44 BPM."
  },
  {
    "objectID": "blog-posts/brain-to-breath.html#music",
    "href": "blog-posts/brain-to-breath.html#music",
    "title": "How Does Brain Focus Influences Your Respiration?",
    "section": "Music",
    "text": "Music\nI chose two tracks that perfectly contrasted each other to observe a range of physiological responses: Powfu’s “Death Bed (Coffee for Your Head)” (lo-fi) and Måneskin’s “Beggin’” (glam rock). The results, analyzed separately for each song, vividly illustrate how my breathing adapted to the energy and mood of the music.\n\nLo-Fi\nWith Powfu’s lo-fi track, the calm and ambient qualities of the music truly settled my breathing. It felt like I was sleeping. My data showed this: my respiration rate (BPM) averaged a calm 11.21, showing remarkable consistency with a minimum of 10.27 BPM and a maximum of 12.33 BPM representing a calmer, more consistent, and generally slower respiratory rate which means I was relaxed. It was clear that the gentle rhythm of the song encouraged a feeling of peace.\n\n\n\nFigure 3. This graph shows the consistent and rhythmic amplitude of my chest movements while listening to Powfu, reflecting relaxed and even breathing, with an average force of 23.6 N.\n\n\n\nFigure 4. This graph illustrates the steady and lower breathing rate (BPM) maintained throughout the Powfu track, with an average of 11.21 BPM and a narrow range between 10.27 BPM and 12.33 BPM, a clear sign of a calm physiological state.\n\n\n\n\nGlam Rock\nIn contrast, when Måneskin’s “Beggin’” started, my breathing shifted dramatically. This glam rock anthem, known for its powerful vocals, and high energy, immediately got me “vibing”. My respiratory patterns became more dramatic and responsive. While the average respiratory force (amplitude) was 18.53 N, the graph clearly shows how the force of my breaths intensified and varied significantly, with peaks reaching 30.98 N and quick drops to a minimum of 9.86 N, demonstrating the erratic nature as I subconsciously mirrored the song’s energy. Concurrently, my breathing rate (BPM) showed an average of 13.59, with significant fluctuations ranging from a minimum of 8.95 BPM to a maximum of 17.75 BPM, reflecting the electrifying and energetic nature of the music. This powerful shift in my breathing was a direct physiological response to the song’s exhilarating impact.\n\n\n\nFigure 5. Here, the “force” or amplitude of my chest movements during Måneskin’s “Beggin’” demonstrates higher and more erratic peaks, indicating more energetic and less controlled breathing. The force varied widely, from a minimum of 9.86 N to a peak of 30.98 N.\n\n\n\nFigure 6. This graph shows my increased and less consistent breathing rate (BPM) while listening to “Beggin’,” reflecting the song’s stimulating effect on my physiology. The rate averaged 13.59 BPM but showed significant variations, from a low of 8.95 BPM to a high of 17.75 BPM."
  },
  {
    "objectID": "blog-posts/convlstm.html",
    "href": "blog-posts/convlstm.html",
    "title": "Understanding ConvLSTM",
    "section": "",
    "text": "In the world of deep learning, we’ve witnessed incredible breakthroughs. Convolutional Neural Networks (CNNs) have revolutionized the way we interpret images and spatial data, while Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, have become indispensable for understanding sequential information like text and time series.\nBut what happens when your data is both spatial and sequential? Think about video: it’s a sequence of images. How do you analyze dynamic patterns that evolve in both space and time? This is where traditional networks often struggle, and it’s precisely the challenge that ConvLSTM was designed to address.\nIn this blog post, we will understand the fundamental building blocks, understand their individual limitations, and then the solution that ConvLSTM offers for understanding dynamic data."
  },
  {
    "objectID": "blog-posts/convlstm.html#cnns",
    "href": "blog-posts/convlstm.html#cnns",
    "title": "Understanding ConvLSTM",
    "section": "CNNs",
    "text": "CNNs\nCNNs are a specialized type of neural network designed for processing grid-like data. This makes them exceptionally good at handling images, video frames, and other spatial datasets. Their power comes from their ability to automatically learn hierarchical spatial features directly from raw input (e.g., pixels). This is done in two steps:\n1. Early layers detect specific local patterns like edges, corners and textures.\n2. Deeper layers then combine these to recognize complex objects and patterns.\n\n\n\nFigure 1: A conceptual diagram of a Convolutional Neural Network, showing how spatial features are extracted and downsampled through convolutional and pooling layers.\n\n\nAs you can see in Figure 1, CNNs apply filters (kernels) across the input to create feature maps, progressively reducing the spatial dimensions while increasing feature complexity. This architecture is inherently designed to understand where features are located in space."
  },
  {
    "objectID": "blog-posts/convlstm.html#lstms",
    "href": "blog-posts/convlstm.html#lstms",
    "title": "Understanding ConvLSTM",
    "section": "LSTMs",
    "text": "LSTMs\nLSTMs are a special type of Recurrent Neural Network (RNN) specifically designed to process sequential data. They were introduced to mitigate common problems in standard RNNs, such as the vanishing or exploding gradient problem, which made it difficult for RNNs to learn long-term dependencies.\nThe core of an LSTM’s power lies in its Cell State (Memory) and its “gates”:\n\nCell State (Memory): Imagine this as a “conveyor belt” for information, running through the entire sequence. It carries information across time steps, allowing the network to retain relevant data for long periods.\nGates: Three specialized “gates” control the flow of information into and out of the cell state:\n\nForget Gate: Decides what information to discard from the previous cell state.\nInput Gate: Determines how much of the new candidate information (derived from the current input and previous hidden state) should be added to the cell state.\nOutput Gate: Controls how much of the current cell state will contribute to the hidden state, which then serves as the output for the current time step and input for the next.\n\n\n\n\n\nFigure 2: The internal structure of a Long Short-Term Memory (LSTM) cell.\n\n\nFigure 2 visually represents these gates and how they interact to selectively update and output information, making LSTMs adept at remembering crucial details over long sequences."
  },
  {
    "objectID": "blog-posts/convlstm.html#cnns-good-at-space-but-bad-at-time",
    "href": "blog-posts/convlstm.html#cnns-good-at-space-but-bad-at-time",
    "title": "Understanding ConvLSTM",
    "section": "CNNs: Good at Space but Bad at Time",
    "text": "CNNs: Good at Space but Bad at Time\nStandard CNNs, while excellent at extracting features from individual images or frames but they do not inherently capture temporal dependencies between consecutive frames. When you feed a sequence of images (like a video) into a standard CNN, it treats each frame as an independent input. This means it can recognize objects within each frame, but it loses crucial information about motion, changes, or the sequence of events over time. For example, a CNN could identify a car in two consecutive frames, but it wouldn’t inherently understand that the car is moving or how it’s moving."
  },
  {
    "objectID": "blog-posts/convlstm.html#lstms-good-at-time-but-bad-at-space",
    "href": "blog-posts/convlstm.html#lstms-good-at-time-but-bad-at-space",
    "title": "Understanding ConvLSTM",
    "section": "LSTMs: Good at Time but Bad at Space",
    "text": "LSTMs: Good at Time but Bad at Space\nTraditional LSTMs are excellent for sequences, but they expect a 1D vector as input at each time step. This presents a major challenge for spatial data like images or video frames. To feed an image (e.g., a 64x64 pixel grayscale image) into a standard LSTM, you would first have to “flatten” (unroll) its 2D grid into a long 1D vector (e.g., 4096 pixels).\nThis “flattening” process leads to two significant problems:\n\nLoss of Spatial Relationships: When you flatten an image, you destroy the crucial spatial relationships between pixels. Pixels that were close together in the 2D grid become distant in the 1D vector. The LSTM then loses the ability to recognize patterns based on proximity, adjacency, or overall shape – the very strengths of CNNs.\nHigh Dimensionality: For higher-resolution images or colored images, flattening can lead to extremely long input vectors, making the LSTM computationally expensive, prone to overfitting, and harder to train effectively."
  },
  {
    "objectID": "blog-posts/convlstm.html#the-5d-input",
    "href": "blog-posts/convlstm.html#the-5d-input",
    "title": "Understanding ConvLSTM",
    "section": "The 5D Input",
    "text": "The 5D Input\nBefore diving into the mechanics, let’s understand the typical input data shape for a ConvLSTM layer. It’s usually a 5D tensor with the following dimensions:\n(batch_size, timesteps, height, width, channels)\nLet’s break down each dimension:\n\nbatch_size: The number of independent sequences processed simultaneously (for parallel computation).\ntimesteps: The length of the sequence (e.g., the number of video frames in a clip).\nheight: The spatial height of each input frame/grid.\nwidth: The spatial width of each input frame/grid.\nchannels: The number of feature channels for each input frame/grid (e.g., 3 for an RGB image, 1 for grayscale).\n\nThis 5D structure is crucial because it allows the ConvLSTM to operate on the full spatial dimensions of your data at each time step."
  },
  {
    "objectID": "blog-posts/convlstm.html#convolutions-inside-the-lstm-loop",
    "href": "blog-posts/convlstm.html#convolutions-inside-the-lstm-loop",
    "title": "Understanding ConvLSTM",
    "section": "Convolutions Inside the LSTM loop",
    "text": "Convolutions Inside the LSTM loop\nIn a traditional LSTM, the gates (Forget, Input, Output) and the candidate cell state generation perform matrix multiplications with their inputs (current input and previous hidden state).\nIn ConvLSTM, all these matrix multiplications are fundamentally replaced by convolutional operations. This means that ,the Forget Gate, the Input Gate, the Output Gate and the candidate cell state generation all utilises 2D or 3D, depending on your data and implementation convolutional filters. This is how CNNs work inside the LSTM loop.\n\n\n\nFigure 3: A conceptual diagram of a ConvLSTM cell, highlighting how all internal matrix multiplications are replaced by convolutional operations (denoted by the orange asterisk)\n\n\nAs shown in Figure 3, the inputs \\(X_t\\), \\(H_{t-1}\\), and \\(C_{t-1}\\) are no longer flattened vectors. Instead, they are spatial feature maps (or the raw image/frame), and the weights (\\(W_{XC}\\), \\(W_{XH}\\), etc.) are now convolutional kernals. These kernels slide across the spatial dimensions of the input, preserving the spatial hierarchy."
  },
  {
    "objectID": "blog-posts/convlstm.html#preserving-spatial-hierarchy-and-learning-temporal-dependencies",
    "href": "blog-posts/convlstm.html#preserving-spatial-hierarchy-and-learning-temporal-dependencies",
    "title": "Understanding ConvLSTM",
    "section": "Preserving Spatial Hierarchy and Learning Temporal Dependencies",
    "text": "Preserving Spatial Hierarchy and Learning Temporal Dependencies\nBy replacing matrix multiplications with convolutions, ConvLSTM achieves two critical advantages:\n\nSpatial Feature Learning: Just like a CNN, it can learn and extract spatial features (edges, textures, object parts) from each input frame or spatial slice at every time step.\n\nTemporal Dependency Modeling: Like an LSTM, it can maintain and update an internal cell state across time, allowing it to learn long-term dependencies and patterns in the sequence.\n\nThis means that instead of just learning what to remember or forget (as in a regular LSTM), a ConvLSTM learns what spatial patterns to remember or forget over time, and how those patterns evolve."
  },
  {
    "objectID": "blog-posts/git-action.html",
    "href": "blog-posts/git-action.html",
    "title": "Explore GitHub Beyond Codes",
    "section": "",
    "text": "Introduction: GitHub - More Than Just Code\nWhen you hear “GitHub” you probably think of code repositories, pull requests, and software development. And you aren’t wrong! GitHub is undeniably the world’s leading platform for collaborative software development.\nBut what if I told you GitHub’s capabilities are beyond these lines of code? What if you could leverage this powerful platform to host your resume, showcase your portfolio, share academic papers, or even create a simple personal website – all for free?\nGitHub is an incredible tool that can serve as your ultimate digital portfolio, allowing you to present a wide range of your work to the world. And if you’re already using Quarto for your blog (like me!), you’re in an even better position to showcase a diverse range of content effortlessly. In this blog post, we’ll explore how you can use GitHub to showcase more than just your programming.\n\n\nWhy Use GitHub for Your Non-Code Assets?\nBefore diving into the “how,” let’s consider the “why”:\n\nFree Hosting: GitHub Pages offers completely free web hosting for static content. This means you can host your CV, PDF portfolio, or a simple personal website without paying a dime.\nVersion Control: GitHub’s version control (Git) allows you to track changes, revert to previous versions, and manage updates seamlessly. No more “resume_final_v2_really_final.pdf”!\nAccessibility: Your files are hosted online and accessible from anywhere with an internet connection. Share a simple URL with potential employers, collaborators, or friends.\nProfessionalism: A well-organized GitHub repository, especially when paired with a custom GitHub Pages site (perhaps built with Quarto!) shows attention to detail and dedication.\nSimplicity: For basic hosting, setting up GitHub Pages is surprisingly straightforward, often requiring minimal to no coding knowledge. For those already using Quarto, much of the setup is already handled for you!\n\n\n\nWhat Can You Host on GitHub (Beyond Code)?\nHere are some popular non-code assets you can effectively host:\n\nResumes & CVs (PDF, HTML): The most common use case. Make your resume easily shareable and always up-to-date online. You can even create an interactive HTML resume!\nPortfolios (PDF, Images, HTML):\n\nDesign Portfolios: Showcase your graphic design, UI/UX work, illustrations, or photography. You can embed images or link to PDF portfolios.\nWriting Portfolios: Host PDFs of articles, essays, short stories, or even link to blog posts.\n\nAcademic Papers & Presentations (PDF, HTML): Researchers and students can share their work, theses, or presentation slides. Quarto is excellent for creating academic documents!\nReports & Documents (PDF): Any general document you want to share publicly.\nPersonal Websites/Blogs: Use Quarto, Jekyll, or simply plain HTML/CSS to build a simple, elegant personal site without complex server setup.\nData Visualizations: If you create interactive visualizations (e.g., with D3.js or Plotly.js), GitHub Pages is an excellent way to host and share them. Quarto supports embedding these beautifully.\nInteractive Demos: For front-end developers, host simple web-based demos of your projects.\n\n\n\nHow to Host Your Assets with GitHub Pages\nThe magic behind hosting non-code assets lies with GitHub Pages. It is a service that takes files directly from a GitHub repository, runs them through a static site generator (like Quarto or Jekyll, though not always necessary), and publishes a website.\nHere’s a simplified general workflow, keeping in mind that Quarto often streamlines much of this for you:\n\nCreate a Repository:\n\nLog in to GitHub and create a new public repository. Give it a descriptive name like my-portfolio, kirtangangani-resume, or personal-website.\nNote: For a user or organization site, name your repository exactly username.github.io (e.g., kirtangangani.github.io). This automatically sets up your GitHub Pages URL.\n\nUpload Your Files:\n\nYou can drag and drop files directly into your new repository via the GitHub web interface.\nAlternatively, clone the repository to your local machine, add your files, and then push them using Git commands (e.g., git add ., git commit -m \"Add resume\", git push origin main).\nFor a simple PDF, just upload the PDF file (e.g., resume.pdf).\nFor an HTML website, make sure your main file is named index.html in the root of your repository.\n\nEnable GitHub Pages:\n\nGo to your repository’s Settings tab.\nOn the left sidebar, click on Pages.\nUnder “Build and deployment,” choose your branch (usually main or master) as the “Source.”\nClick “Save.”\nGitHub will then provide you with a URL where your content is published (e.g., https://username.github.io/repository-name/). For a user/organization site (username.github.io), the URL will just be https://username.github.io.\n\nShare Your Link!\n\nOnce GitHub finishes deploying (it might take a minute or two), your content will be live! You can then share this URL with anyone.\n\n\nExample for a PDF CV:\n\nCreate a repo named my-resume.\nUpload kirtan_gangani_cv.pdf to the root of the repo.\nGo to Settings &gt; Pages, select main branch, and save.\nYour CV will be accessible at https://yourusername.github.io/my-resume/kirtan_gangani_cv.pdf.\n\n\n\nTips for Maximizing Your Digital Portfolio on GitHub:\n\nREADME.md: Always include a clear and descriptive README.md file in your repository. Explain what the repo contains, what files are available, and how to access them (e.g., provide direct links to your hosted PDFs).\nOrganize Folders: For larger portfolios, use well-named folders (e.g., /design-projects, /writing-samples, /presentations).\nCustom Domains: For a more professional touch, you can even connect a custom domain (e.g., www.yourname.com) to your GitHub Pages site.\nUse Quarto’s Power: If you’re hosting an index.qmd or similar, Quarto can render beautiful HTML pages from Markdown, R, Python, or Julia code, making it perfect for dynamic portfolios or academic papers.\nCommit Your changes: Commit changes to your resume or portfolio as you update them. This creates a history you can always refer back to.\n\n\n\nConclusion\nGitHub is no longer just for developers. It’s a powerful, free, and accessible platform for anyone looking to establish a robust online presence. Whether you are a student building your first online resume, a designer showcasing your creative flair, or a writer sharing your prose, GitHub Pages (especially with the help of Quarto!) provides the perfect stage.\nSince you’re reading this on my Quarto-powered blog, you can see firsthand how effective this setup is for presenting diverse content!"
  },
  {
    "objectID": "blog-posts/markov-chain.html",
    "href": "blog-posts/markov-chain.html",
    "title": "Understanding Markov Chains",
    "section": "",
    "text": "Markov chains, named after Andrey Markov is a powerful mathematical model used to show a sequence of events where the probability of each event depends only on the state achieved in the previous event. It doesn’t care about the entire history that led to the current state, just the current state itself. This unique “memoryless” property makes them surprisingly versatile and useful for modeling a wide range of real-world phenomena."
  },
  {
    "objectID": "blog-posts/markov-chain.html#states",
    "href": "blog-posts/markov-chain.html#states",
    "title": "Understanding Markov Chains",
    "section": "States",
    "text": "States\nStates are all the possible “situations” or “conditions” our system can be in. For example:\n\nWeather: Sunny, Cloudy, Rainy\n\nA Stock Price: Up, Down, Stagnant\n\nYour Mood: Happy, Neutral, Sad"
  },
  {
    "objectID": "blog-posts/markov-chain.html#transitions-and-probabilities",
    "href": "blog-posts/markov-chain.html#transitions-and-probabilities",
    "title": "Understanding Markov Chains",
    "section": "Transitions and Probabilities",
    "text": "Transitions and Probabilities\nA transition is simply the movement from one state to another. For example, the weather changing from “Sunny” to “Cloudy.”\nEach transition comes with a transition probability, which is the likelihood of moving from a current state to a next state. These probabilities are always between 0 and 1.\nNote: For any given state in a system, the sum of the probabilities of all possible transitions originating from that state (including the probability of transitioning back to itself) must always equal 1. This holds true regardless of the total number of states in the system."
  },
  {
    "objectID": "blog-posts/markov-chain.html#the-markov-property",
    "href": "blog-posts/markov-chain.html#the-markov-property",
    "title": "Understanding Markov Chains",
    "section": "The Markov Property",
    "text": "The Markov Property\nThe Markov Property means that the probability of moving to a future state depends only on the current state, and not on any of the states that came before it.\nImagine you’re playing Monopoly and you land on a specific square, it doesn’t matter if you got there by rolling 6 and 2, or by rolling 5 and 3. All that matters is that you are currently on that square. This “forgetfulness” of past events is what makes Markov Chains unique and, surprisingly, easier to model."
  },
  {
    "objectID": "blog-posts/markov-chain.html#discrete-time-vs.-continuous-time",
    "href": "blog-posts/markov-chain.html#discrete-time-vs.-continuous-time",
    "title": "Understanding Markov Chains",
    "section": "Discrete-Time vs. Continuous-Time",
    "text": "Discrete-Time vs. Continuous-Time\n\nDiscrete-Time Markov Chains (DTMC): In a DTMC, transitions between states occur at fixed, regular intervals or “steps.” Think of it like taking a snapshot of the system every hour, day, or year. Our weather example is a perfect DTMC because we’re looking at the weather “tomorrow” based on “today.” The transition matrix you’ve seen applies directly to DTMCs.\nContinuous-Time Markov Chains (CTMC): In contrast, CTMCs allow transitions to occur at any point in time, not just at predefined intervals. The amount of time spent in a particular state before transitioning is a random variable, often modeled by an exponential distribution. While more complex mathematically, CTMCs are used to model systems where events happen asynchronously, like customer arrivals in a queue or radioactive decay."
  },
  {
    "objectID": "blog-posts/markov-chain.html#discrete-state-vs.-continuous-state",
    "href": "blog-posts/markov-chain.html#discrete-state-vs.-continuous-state",
    "title": "Understanding Markov Chains",
    "section": "Discrete-State vs. Continuous-State",
    "text": "Discrete-State vs. Continuous-State\nThis distinction refers to the nature of the states themselves.\n\nDiscrete-State Markov Chains: This is what we’ve been discussing. The system can only be in a finite (or countably infinite) number of distinct, separate states. Examples include: “Sunny,” “Cloudy,” “Rainy”; “Up,” “Down,” “Stagnant” for stock prices; or even the squares on a Monopoly board. All the examples and the transition matrix we’ve used so far fall into this category.\nContinuous-State Markov Chains: In these chains, the state space is continuous, meaning the system can take on any value within a range. For instance, modeling the exact temperature (e.g., 25.3 degrees, 25.31 degrees, etc.) or the precise stock price value over time. These are often more complex and are typically modeled using stochastic differential equations, moving beyond simple transition matrices. For an introductory understanding, focusing on discrete-state chains is appropriate."
  },
  {
    "objectID": "blog-posts/prng-algorithm.html",
    "href": "blog-posts/prng-algorithm.html",
    "title": "PseudoRandom Number Generator (PRNG)",
    "section": "",
    "text": "A PRNG algorithm or PseudoRandom Number Generator algorithm uses mathematical formulas to produce a sequence of random numbers that approximates the properties of random numbers.\nWhile these numbers are not truly random (as they are deterministic and can be reproduced if the starting state is known), they are designed to be unpredictable and uniformly distributed, making them suitable for a wide range of applicaions."
  },
  {
    "objectID": "blog-posts/prng-algorithm.html#seed",
    "href": "blog-posts/prng-algorithm.html#seed",
    "title": "PseudoRandom Number Generator (PRNG)",
    "section": "Seed",
    "text": "Seed\nThe algorithm starts with an initial value called the seed. This seed determines the entire sequence of numbers that will be generated. For a different sequence, you need a different seed. Often, the current time is used as a seed to make the sequences appear more random."
  },
  {
    "objectID": "blog-posts/prng-algorithm.html#state",
    "href": "blog-posts/prng-algorithm.html#state",
    "title": "PseudoRandom Number Generator (PRNG)",
    "section": "State",
    "text": "State\nThe PRNG maintains an internal “state” that changes with each number generated. This state is the crucial input to the transformation function, and it is updated after each number is produced, becoming the basis for the next calculation."
  },
  {
    "objectID": "blog-posts/prng-algorithm.html#transformation-function",
    "href": "blog-posts/prng-algorithm.html#transformation-function",
    "title": "PseudoRandom Number Generator (PRNG)",
    "section": "Transformation Function",
    "text": "Transformation Function\nA mathematical function is applied to the current state to produce the next pseudorandom number and update the internal state for the subsequent generation."
  },
  {
    "objectID": "blog-posts/prng-algorithm.html#period",
    "href": "blog-posts/prng-algorithm.html#period",
    "title": "PseudoRandom Number Generator (PRNG)",
    "section": "Period",
    "text": "Period\nPRNGs eventually repeat their sequence of numbers. The length of this sequence before it repeats is called the “period.” A good PRNG has a very long period."
  },
  {
    "objectID": "blog-posts/understanding-lstm.html",
    "href": "blog-posts/understanding-lstm.html",
    "title": "Understanding the LSTM Cell",
    "section": "",
    "text": "Imagine trying to understand a long conversation where you forget the beginning halfway through. Traditional Recurrent Neural Network (RNN) models often struggle with this “memory” problem when dealing with sequences of data like text, speech, or time series. As sequences get longer, the information from earlier steps gets “diluted” or “forgotten” or in other words, gradients become too small which is called gradient vanishing. This makes it hard for the RNN to learn long-term dependencies meaning that they can’t effectively connect information from far earlier parts of a sequence to make a current decision.\nThis is where Long Short-Term Memory (LSTM) networks comes in. LSTMs are a special type of RNNs designed to overcome this limitation, allowing AI to remember important information over long periods. They were introduced to mitigate gradient vanishing/exploding problem faced by standard RNNs.\nThis is achieved by a “cell state” that acts like a conveyor belt, carrying information through the network, allowing it to preserve information over long sequences. This is their “long-term memory.” LSTMs achieve their long-term memory capabilities through a unique internal structure called “gates.” These gates are like intelligent filters that control the flow of information in and out of the memory cell. More about gate will be explained in later sections."
  },
  {
    "objectID": "blog-posts/understanding-lstm.html#forget-gate",
    "href": "blog-posts/understanding-lstm.html#forget-gate",
    "title": "Understanding the LSTM Cell",
    "section": "Forget Gate",
    "text": "Forget Gate\nThe forget gate’s primary role is to decide what information from the previous cell state (c_{t-1}) should be discarded or “forgotten”. It takes the current input (x_t) and the hidden state from the last time step (h_{t-1}) as inputs. These are passed through a sigmoid activation function.\n\\[\\begin{aligned}\nf_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n\\end{aligned}\\]\nThe output f_t is a vector with values from 0 to 1. This output is then element-wise multiplied with the previous cell state (c_{t-1}). A value of 1 means “keep all of this information”, while a value of 0 means “forget all of this informtion”. A value of 0.5 would indicate keep half of that information."
  },
  {
    "objectID": "blog-posts/understanding-lstm.html#input-gate",
    "href": "blog-posts/understanding-lstm.html#input-gate",
    "title": "Understanding the LSTM Cell",
    "section": "Input Gate:",
    "text": "Input Gate:\nThe input gate is responsible for determining which new information from the current input should be stored in the cell state. It works in two parts:\n\nCandidate Memory (\\(\\tilde{c}_t\\))\nBefore the input gate itself, there’s the candidate memory, often denoted as _t. Its purpose is to propose new information that could be added to the cell state. Unlike the gates, which use sigmoid, the candidate memory uses a hyperbolic tangent (tanh) activation function. The tanh function outputs values between -1 and 1, allowing for both positive and negative contributions to the cell state.\n\\[\\begin{aligned}\n\\tilde{c}_t &= \\tanh(W_c [h_{t-1}, x_t] + b_c)\n\\end{aligned}\\]\n\n\nInput gate (\\(i_t\\))\nThe input gate then decides how much of this newly proposed candidate memory t should actually be added to the cell state. Similar to the forget gate, it takes the current input (x_t) and the previous hidden state (h{t-1})and passes them through a sigmoid activation function.\n\\[\\begin{aligned}\ni_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n\\end{aligned}\\]\nthe output i_t acts as a filter for the candidate memory _t\n\n\nCell state Update (\\(c_t\\))\nThis is where the magic happens for updating the long-term memory of the network. The new cell state (\\(c_t\\)) is combination of two components:\n1. The information from the previous cell state (\\(c_{t−1}\\)) that the forget gate (\\(f_t\\)) decided to keep.\n2. The new candidate memory (\\(\\tilde{c}_t\\)) that the input gate (\\(i_t\\)) decided to add.\n\\[\\begin{aligned}\nc_t &= f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t\n\\end{aligned}\\]\nThese two parts are element-wise added to form the updared cell state (\\(c_t\\)).\nThis updated cell state \\(c_t\\) then carries the network’s long-term memory forward to the next time step."
  },
  {
    "objectID": "blog-posts/understanding-lstm.html#output-gate",
    "href": "blog-posts/understanding-lstm.html#output-gate",
    "title": "Understanding the LSTM Cell",
    "section": "Output Gate",
    "text": "Output Gate\nFinally, the output gate controls how much of the updated cell state (\\(c_t\\)) will be exposed as the current hidden state (\\(h_t\\)). The hidden state is the output of the LSTM cell at the current time step and is also passed on to the next time step.\nFirst, the output gate determines which parts of the cell state are relevant for the current hidden state. It uses the current input (\\(x_t\\)) and the previous hidden state (\\(h_{t−1}\\)) passed through a sigmoid function:\n\\[\\begin{aligned}\no_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n\\end{aligned}\\]\nNext, the updated cell state (\\(c_t\\)) is passed through a tanh activation function. This scales the cell state values to between -1 and 1, making them ready to be filtered.\nFinally, the result of the tanh operation on \\(c_t\\) is element-wise multiplied by the output gate’s activation (\\(o_t\\)) to produce the new hidden state (\\(h_t\\)):\n\\[\\begin{aligned}\nh_t &= o_t \\cdot \\tanh(c_t)\n\\end{aligned}\\]\nThe hidden state \\(h_t\\) serves as the output of the LSTM block for the current time step and is also used as an input to the gates in the next time step."
  },
  {
    "objectID": "blog-posts/understanding-lstm.html#advantages-of-lstms",
    "href": "blog-posts/understanding-lstm.html#advantages-of-lstms",
    "title": "Understanding the LSTM Cell",
    "section": "Advantages of LSTMs:",
    "text": "Advantages of LSTMs:\n\nSolving the Vanishing Gradient Problem: This is their most significant advantage. LSTMs effectively address the vanishing gradient problem inherent in traditional RNNs, allowing them to learn and retain information over very long sequences. This is primarily due to their unique cell state and gate mechanisms that regulate information flow.\nCapturing Long-Term Dependencies: Thanks to their ability to maintain a persistent cell state, LSTMs can connect information from distant past steps to make decisions in the present. This is crucial for tasks like understanding context in long sentences or predicting future values in time series based on historical trends.\nHandling Variable-Length Sequences: LSTMs are naturally designed to process sequences of varying lengths, making them highly versatile for real-world data like text (sentences of different lengths), speech (utterances of different durations), and time series.\nRobustness to Noise (to some extent): The gating mechanism allows LSTMs to selectively filter out irrelevant or noisy information, focusing on the most important features in the sequence.\nWide Applicability: LSTMs have found immense success across a broad spectrum of domains, including:\n\nNatural Language Processing (NLP): Machine translation, sentiment analysis, text summarization, named entity recognition, language modeling.\nSpeech Recognition: Converting spoken words into text.\nTime Series Forecasting: Predicting stock prices, weather patterns, energy consumption.\nVideo Processing: Action recognition, video captioning."
  },
  {
    "objectID": "blog-posts/understanding-lstm.html#disadvantages-of-lstms",
    "href": "blog-posts/understanding-lstm.html#disadvantages-of-lstms",
    "title": "Understanding the LSTM Cell",
    "section": "Disadvantages of LSTMs:",
    "text": "Disadvantages of LSTMs:\n\nComputational Cost: LSTMs are more computationally intensive and slower to train compared to simpler neural networks or even standard RNNs. This is due to the increased number of parameters (weights and biases for each gate) and the complex calculations involved in their internal mechanisms.\nComplex Architecture: The multiple gates and intricate interactions within an LSTM cell make them more complex to understand and debug compared to simpler models. While powerful, this complexity can be a barrier for newcomers.\nDifficulty with Very Long Sequences: Although LSTMs significantly mitigate the long-term dependency problem, they can still struggle with extremely long sequences. As the sequence length increases, the computational burden grows, and even LSTMs can start to lose efficiency or effectiveness.\nLimited Parallelization: The inherent sequential nature of LSTMs (processing one time step after another) makes it challenging to fully parallelize their training across multiple computing cores or GPUs, especially during the forward and backward passes within a sequence. This is a key area where newer architectures like Transformers have shown significant advantages.\nHyperparameter Tuning: LSTMs often require careful tuning of hyperparameters (e.g., number of hidden units, learning rate, dropout) to achieve optimal performance, which can be a time-consuming process.\nOutshined by Transformers in Many NLP Tasks: For many state-of-the-art NLP tasks, transformer architectures (which use attention mechanisms instead of recurrence) have largely surpassed LSTMs in performance, particularly for very long sequences and tasks requiring complex contextual understanding."
  }
]