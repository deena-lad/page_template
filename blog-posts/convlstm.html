<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kirtan Gangani">
<meta name="dcterms.date" content="2025-07-13">

<title>Understanding ConvLSTM – Homepage</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bb05f5d0074da2129d766b238819cd35.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Homepage</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../blog-posts/index.html"> 
<span class="menu-text">Blogposts</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Kirtan7311"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kirtan-gangani-k789/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-building-blocks-cnns-and-lstms" id="toc-the-building-blocks-cnns-and-lstms" class="nav-link" data-scroll-target="#the-building-blocks-cnns-and-lstms">The Building Blocks: CNNs and LSTMs</a>
  <ul class="collapse">
  <li><a href="#cnns" id="toc-cnns" class="nav-link" data-scroll-target="#cnns">CNNs</a></li>
  <li><a href="#lstms" id="toc-lstms" class="nav-link" data-scroll-target="#lstms">LSTMs</a></li>
  </ul></li>
  <li><a href="#the-limitations-of-individual-networks" id="toc-the-limitations-of-individual-networks" class="nav-link" data-scroll-target="#the-limitations-of-individual-networks">The Limitations of Individual Networks</a>
  <ul class="collapse">
  <li><a href="#cnns-good-at-space-but-bad-at-time" id="toc-cnns-good-at-space-but-bad-at-time" class="nav-link" data-scroll-target="#cnns-good-at-space-but-bad-at-time">CNNs: Good at Space but Bad at Time</a></li>
  <li><a href="#lstms-good-at-time-but-bad-at-space" id="toc-lstms-good-at-time-but-bad-at-space" class="nav-link" data-scroll-target="#lstms-good-at-time-but-bad-at-space">LSTMs: Good at Time but Bad at Space</a></li>
  </ul></li>
  <li><a href="#the-solution-convlstm" id="toc-the-solution-convlstm" class="nav-link" data-scroll-target="#the-solution-convlstm">The Solution: ConvLSTM</a></li>
  <li><a href="#how-convlstm-works" id="toc-how-convlstm-works" class="nav-link" data-scroll-target="#how-convlstm-works">How ConvLSTM Works</a>
  <ul class="collapse">
  <li><a href="#the-5d-input" id="toc-the-5d-input" class="nav-link" data-scroll-target="#the-5d-input">The 5D Input</a></li>
  <li><a href="#convolutions-inside-the-lstm-loop" id="toc-convolutions-inside-the-lstm-loop" class="nav-link" data-scroll-target="#convolutions-inside-the-lstm-loop">Convolutions Inside the LSTM loop</a></li>
  <li><a href="#preserving-spatial-hierarchy-and-learning-temporal-dependencies" id="toc-preserving-spatial-hierarchy-and-learning-temporal-dependencies" class="nav-link" data-scroll-target="#preserving-spatial-hierarchy-and-learning-temporal-dependencies">Preserving Spatial Hierarchy and Learning Temporal Dependencies</a></li>
  </ul></li>
  <li><a href="#simple-implementation-on-convlstm-next-frame-prediction" id="toc-simple-implementation-on-convlstm-next-frame-prediction" class="nav-link" data-scroll-target="#simple-implementation-on-convlstm-next-frame-prediction">Simple Implementation on ConvLSTM Next Frame Prediction</a></li>
  <li><a href="#real-world-applications-of-convlstm" id="toc-real-world-applications-of-convlstm" class="nav-link" data-scroll-target="#real-world-applications-of-convlstm">Real-World Applications of ConvLSTM</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Understanding ConvLSTM</h1>
  <div class="quarto-categories">
    <div class="quarto-category">CNN</div>
    <div class="quarto-category">LSTM</div>
  </div>
  </div>



<div class="quarto-title-meta column-page-left">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kirtan Gangani </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In the world of deep learning, we’ve witnessed incredible breakthroughs. <strong>Convolutional Neural Networks (CNNs)</strong> have revolutionized the way we interpret <strong>images and spatial data</strong>, while <strong>Recurrent Neural Networks (RNNs)</strong>, particularly <strong>Long Short-Term Memory (LSTM) networks</strong>, have become indispensable for understanding <strong>sequential information</strong> like text and time series.</p>
<p>But what happens when your data is both spatial and sequential? Think about video: it’s a sequence of images. How do you analyze dynamic patterns that evolve in both space and time? This is where traditional networks often struggle, and it’s precisely the challenge that ConvLSTM was designed to address.</p>
<p>In this blog post, we will understand the fundamental building blocks, understand their individual limitations, and then the solution that ConvLSTM offers for understanding dynamic data.</p>
</section>
<section id="the-building-blocks-cnns-and-lstms" class="level1">
<h1>The Building Blocks: CNNs and LSTMs</h1>
<p>Let’s first quickly revisit its two powerful parents.</p>
<section id="cnns" class="level2">
<h2 class="anchored" data-anchor-id="cnns">CNNs</h2>
<p>CNNs are a specialized type of neural network designed for <strong>processing grid-like data</strong>. This makes them exceptionally good at handling images, video frames, and other spatial datasets. Their power comes from their ability to automatically <strong>learn</strong> hierarchical spatial features <strong>directly from raw input</strong> (e.g., pixels). This is done in two steps:<br>
1. Early layers detect <strong>specific local patterns</strong> like edges, corners and textures.<br>
2. Deeper layers then combine these to <strong>recognize complex objects and patterns</strong>.</p>
<dl>
<dt><img src="./images/cnn.png" class="img-fluid"></dt>
<dd>
<p>Figure 1: A conceptual diagram of a Convolutional Neural Network, showing how spatial features are extracted and downsampled through convolutional and pooling layers.</p>
</dd>
</dl>
<p>As you can see in Figure 1, CNNs apply <strong>filters</strong> (kernels) across the input to create <strong>feature maps</strong>, progressively reducing the spatial dimensions while increasing feature complexity. This architecture is inherently designed to understand where features are located in space.</p>
</section>
<section id="lstms" class="level2">
<h2 class="anchored" data-anchor-id="lstms">LSTMs</h2>
<p>LSTMs are a special type of Recurrent Neural Network (RNN) specifically designed to <strong>process sequential data</strong>. They were introduced to <strong>mitigate</strong> common problems in standard RNNs, such as the <strong>vanishing or exploding gradient problem</strong>, which made it difficult for RNNs to learn long-term dependencies.</p>
<p>The core of an LSTM’s power lies in its Cell State (Memory) and its “gates”:</p>
<ul>
<li><strong>Cell State (Memory)</strong>: Imagine this as a “conveyor belt” for information, running through the entire sequence. It carries information across time steps, allowing the network to retain relevant data for long periods.</li>
<li><strong>Gates</strong>: Three specialized “gates” control the flow of information into and out of the cell state:
<ul>
<li><strong>Forget Gate</strong>: Decides what information to discard from the previous cell state.</li>
<li><strong>Input Gate</strong>: Determines how much of the new candidate information (derived from the current input and previous hidden state) should be added to the cell state.</li>
<li><strong>Output Gate</strong>: Controls how much of the current cell state will contribute to the hidden state, which then serves as the output for the current time step and input for the next.</li>
</ul></li>
</ul>
<dl>
<dt><img src="./images/lstm-cell.png" class="img-fluid"></dt>
<dd>
<p>Figure 2: The internal structure of a Long Short-Term Memory (LSTM) cell.</p>
</dd>
</dl>
<p>Figure 2 visually represents these gates and how they interact to selectively update and output information, making LSTMs adept at <strong>remembering crucial details over long sequences</strong>.</p>
</section>
</section>
<section id="the-limitations-of-individual-networks" class="level1">
<h1>The Limitations of Individual Networks</h1>
<p>While <strong>CNNs and LSTMs</strong> are incredibly powerful in their respective domains, they have significant <strong>limitations</strong> when dealing with data that exhibits both strong spatial and temporal dependencies.</p>
<section id="cnns-good-at-space-but-bad-at-time" class="level2">
<h2 class="anchored" data-anchor-id="cnns-good-at-space-but-bad-at-time">CNNs: Good at Space but Bad at Time</h2>
<p>Standard CNNs, while <strong>excellent at extracting features</strong> from individual images or frames but they <strong>do not inherently capture temporal dependencies between consecutive frames.</strong> When you feed a sequence of images (like a video) into a standard CNN, it treats each frame as an independent input. This means it can recognize objects within each frame, but it loses crucial information about motion, changes, or the sequence of events over time. For example, a CNN could identify a car in two consecutive frames, but it wouldn’t inherently understand that the car is moving or how it’s moving.</p>
</section>
<section id="lstms-good-at-time-but-bad-at-space" class="level2">
<h2 class="anchored" data-anchor-id="lstms-good-at-time-but-bad-at-space">LSTMs: Good at Time but Bad at Space</h2>
<p>Traditional LSTMs are <strong>excellent for sequences</strong>, but they <strong>expect a 1D vector</strong> as input at each time step. This presents a major challenge for spatial data like images or video frames. To feed an image (e.g., a 64x64 pixel grayscale image) into a standard LSTM, you would first have to <strong>“flatten” (unroll)</strong> its 2D grid into a long 1D vector (e.g., 4096 pixels).</p>
<p>This “flattening” process leads to two significant problems:</p>
<ol type="1">
<li><strong>Loss of Spatial Relationships</strong>: When you flatten an image, you destroy the crucial spatial relationships between pixels. Pixels that were close together in the 2D grid become distant in the 1D vector. The LSTM then loses the ability to recognize patterns based on proximity, adjacency, or overall shape – the very strengths of CNNs.</li>
<li><strong>High Dimensionality</strong>: For higher-resolution images or colored images, flattening can lead to extremely long input vectors, making the LSTM computationally expensive, prone to overfitting, and harder to train effectively.</li>
</ol>
</section>
</section>
<section id="the-solution-convlstm" class="level1">
<h1>The Solution: ConvLSTM</h1>
<p>Recognizing the limitations of relying solely on CNNs for temporal tasks or LSTMs for spatial tasks, ConvLSTM emerged as a powerful hybrid architecture. It integrates:</p>
<ul>
<li><strong>Convolutional Neural Networks (CNNs)</strong>: To expertly extract spatial features from grid-like data (like images).</li>
<li><strong>Long Short-Term Memory (LSTMs)</strong>: To learn and remember long-term dependencies in sequential data.</li>
</ul>
<p><strong>ConvLSTM</strong> was specifically developed to <strong>process data that has both spatial and temporal dimensions.</strong> simultaneously. This makes it ideal for sequential data where each data point is a multi-dimensional grid, such as:</p>
<ul>
<li>Video frames</li>
<li>Weather radar data</li>
<li>Medical imaging sequences (e.g., fMRI, dynamic MRI)</li>
</ul>
<p>Crucially, ConvLSTM <strong>maintains the spatial information</strong> throughout its recurrent connections, unlike traditional LSTMs that would flatten the input, thus providing a much more <strong>effective way to model spatio-temporal dynamics</strong>.</p>
</section>
<section id="how-convlstm-works" class="level1">
<h1>How ConvLSTM Works</h1>
<p>The core innovation of ConvLSTM lies in how it adapts the internal operations of an LSTM cell to handle spatial data directly.</p>
<section id="the-5d-input" class="level2">
<h2 class="anchored" data-anchor-id="the-5d-input">The 5D Input</h2>
<p>Before diving into the mechanics, let’s understand the typical input data shape for a ConvLSTM layer. It’s usually a 5D tensor with the following dimensions:</p>
<p><code>(batch_size, timesteps, height, width, channels)</code></p>
<p>Let’s break down each dimension:</p>
<ul>
<li><code>batch_size</code>: The number of independent sequences processed simultaneously (for parallel computation).</li>
<li><code>timesteps</code>: The length of the sequence (e.g., the number of video frames in a clip).</li>
<li><code>height</code>: The spatial height of each input frame/grid.</li>
<li><code>width</code>: The spatial width of each input frame/grid.</li>
<li><code>channels</code>: The number of feature channels for each input frame/grid (e.g., 3 for an RGB image, 1 for grayscale).</li>
</ul>
<p>This 5D structure is crucial because it allows the ConvLSTM to operate on the full spatial dimensions of your data at each time step.</p>
</section>
<section id="convolutions-inside-the-lstm-loop" class="level2">
<h2 class="anchored" data-anchor-id="convolutions-inside-the-lstm-loop">Convolutions Inside the LSTM loop</h2>
<p>In a traditional LSTM, the gates (Forget, Input, Output) and the candidate cell state generation perform matrix multiplications with their inputs (current input and previous hidden state).</p>
<p>In ConvLSTM, all these <strong>matrix multiplications are fundamentally replaced by convolutional operations</strong>. This means that ,the Forget Gate, the Input Gate, the Output Gate and the candidate cell state generation all utilises 2D or 3D, depending on your data and implementation convolutional filters. This is how CNNs work inside the LSTM loop.</p>
<dl>
<dt><img src="./images/convlstm.png" class="img-fluid"></dt>
<dd>
<p>Figure 3: A conceptual diagram of a ConvLSTM cell, highlighting how all internal matrix multiplications are replaced by convolutional operations (denoted by the orange asterisk)</p>
</dd>
</dl>
<p>As shown in Figure 3, the inputs <span class="math inline">\(X_t\)</span>, <span class="math inline">\(H_{t-1}\)</span>, and <span class="math inline">\(C_{t-1}\)</span> are no longer flattened vectors. Instead, they are spatial feature maps (or the raw image/frame), and the weights (<span class="math inline">\(W_{XC}\)</span>, <span class="math inline">\(W_{XH}\)</span>, etc.) are now convolutional kernals. These kernels slide across the spatial dimensions of the input, preserving the spatial hierarchy.</p>
</section>
<section id="preserving-spatial-hierarchy-and-learning-temporal-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="preserving-spatial-hierarchy-and-learning-temporal-dependencies">Preserving Spatial Hierarchy and Learning Temporal Dependencies</h2>
<p>By replacing matrix multiplications with convolutions, ConvLSTM achieves two critical advantages:</p>
<ol type="1">
<li><strong>Spatial Feature Learning</strong>: Just like a CNN, it can learn and extract spatial features (edges, textures, object parts) from each input frame or spatial slice at every time step.<br>
</li>
<li><strong>Temporal Dependency Modeling</strong>: Like an LSTM, it can maintain and update an internal cell state across time, allowing it to learn long-term dependencies and patterns in the sequence.</li>
</ol>
<p>This means that instead of just learning what to remember or forget (as in a regular LSTM), a ConvLSTM learns what spatial patterns to remember or forget over time, and how those patterns evolve.</p>
</section>
</section>
<section id="simple-implementation-on-convlstm-next-frame-prediction" class="level1">
<h1>Simple Implementation on ConvLSTM Next Frame Prediction</h1>
<p>Before we dive into the ConvLSTM implementation, let’s take a look at the dataset. This section uses a short video clip, randomly sourced online. While its exact subject matter is unknown (my best guess involves some form of gaseous movement!), it provides an excellent and visually engaging sequence for our ConvLSTM model.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="./videos/dataset.mp4"></video></div>
<div id="a22c8fee" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageOps</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>src_path <span class="op">=</span> <span class="vs">r'</span><span class="dv">.</span><span class="vs">/dataset/convlstm'</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>length <span class="op">=</span> <span class="bu">len</span>(os.listdir(src_path))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(length):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> Image.<span class="bu">open</span>(<span class="ss">f'</span><span class="sc">{</span>src_path<span class="sc">}</span><span class="ss">/img</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.png'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    img<span class="op">=</span>img.crop((<span class="dv">0</span>,<span class="dv">120</span>,<span class="dv">640</span>,<span class="dv">360</span>))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    img<span class="op">=</span>img.convert(<span class="st">'RGB'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    img<span class="op">=</span>img.resize((<span class="dv">160</span>,<span class="dv">60</span>),Image.Resampling.LANCZOS)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    img<span class="op">=</span>np.array(img)<span class="op">/</span><span class="dv">255</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    img<span class="op">=</span>img.reshape(<span class="dv">60</span>,<span class="dv">160</span>,<span class="dv">3</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    images.append(img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e49c3453" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_test(images,step_size,start,stop):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    seqx<span class="op">=</span>[]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    seqy<span class="op">=</span>[]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>[]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>[]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    sample_size<span class="op">=</span>stop<span class="op">-</span>start</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(start,stop):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        endx<span class="op">=</span>i<span class="op">+</span>step_size</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        endy<span class="op">=</span>i<span class="op">+</span>step_size<span class="op">*</span><span class="dv">2</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        seqx.append(images[i:endx])</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        seqy.append(images[endx:endy])</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        x.append(np.array(seqx))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        y.append(np.array(seqy))</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        seqx.clear()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        seqy.clear()</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(x).reshape(sample_size,step_size, <span class="dv">60</span>, <span class="dv">160</span>, <span class="dv">3</span>),np.array(y).reshape(sample_size,step_size,<span class="dv">60</span>, <span class="dv">160</span>, <span class="dv">3</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>step_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>xtrain,ytrain <span class="op">=</span> train_test(images,step_size,<span class="dv">0</span>,<span class="dv">800</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>xtest,ytest <span class="op">=</span> train_test(images,step_size,<span class="dv">900</span>,<span class="dv">990</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>xval,yval <span class="op">=</span> train_test(images,step_size,<span class="dv">800</span>,<span class="dv">900</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'train: x</span><span class="sc">{</span>xtrain<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,y</span><span class="sc">{</span>ytrain<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">  test: x</span><span class="sc">{</span>xtest<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">,y:</span><span class="sc">{</span>ytest<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="99655d25" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> Sequential</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> ConvLSTM2D,BatchNormalization,Conv3D</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model():</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    seq <span class="op">=</span> Sequential()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    seq.add(ConvLSTM2D(filters<span class="op">=</span><span class="dv">40</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                       input_shape<span class="op">=</span>(step_size, <span class="dv">60</span>, <span class="dv">160</span>, <span class="dv">3</span>),</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="st">'same'</span>, return_sequences<span class="op">=</span><span class="va">True</span>,activation<span class="op">=</span><span class="st">"relu"</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    seq.add(BatchNormalization())</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    seq.add(ConvLSTM2D(filters<span class="op">=</span><span class="dv">40</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="st">'same'</span>, return_sequences<span class="op">=</span><span class="va">True</span>,activation<span class="op">=</span><span class="st">"relu"</span>))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    seq.add(BatchNormalization())</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    seq.add(ConvLSTM2D(filters<span class="op">=</span><span class="dv">40</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="st">'same'</span>, return_sequences<span class="op">=</span><span class="va">True</span>,activation<span class="op">=</span><span class="st">"relu"</span>))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    seq.add(BatchNormalization())</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    seq.add(ConvLSTM2D(filters<span class="op">=</span><span class="dv">40</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="st">'same'</span>, return_sequences<span class="op">=</span><span class="va">True</span>,activation<span class="op">=</span><span class="st">"relu"</span>))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    seq.add(BatchNormalization())</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    seq.add(Conv3D(filters<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span><span class="st">'sigmoid'</span>,</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>                   padding<span class="op">=</span><span class="st">'same'</span>, data_format<span class="op">=</span><span class="st">'channels_last'</span>))</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    seq.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mae'</span>, optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>))</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seq</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>m<span class="op">=</span>model()</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>m.summary()</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>m.fit(xtrain,ytrain,epochs<span class="op">=</span><span class="dv">15</span>,batch_size<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>validation_data<span class="op">=</span>(xval, yval))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4960976c" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predictions(model,xtest,ytest):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    choice <span class="op">=</span> np.random.choice(<span class="bu">len</span>(xtest))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(choice)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    pre <span class="op">=</span> model.predict(xtest[choice].reshape(<span class="dv">1</span>,step_size, <span class="dv">60</span>, <span class="dv">160</span>, <span class="dv">3</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    pre<span class="op">=</span>pre<span class="op">*</span><span class="dv">255</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> time, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes[<span class="dv">0</span>]):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        ax.imshow(np.squeeze(ytest[choice][time]))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Ground Truth </span><span class="sc">{</span>time<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">"off"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> time, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes[<span class="dv">1</span>]):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        im<span class="op">=</span>Image.fromarray(np.uint8(pre[<span class="dv">0</span>][time]),<span class="st">'RGB'</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        ax.imshow(im)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Prediction </span><span class="sc">{</span>time<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">"off"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>predictions(m,xtest,ytest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="real-world-applications-of-convlstm" class="level1">
<h1>Real-World Applications of ConvLSTM</h1>
<p>The ability of ConvLSTM to concurrently model spatial and temporal dynamics makes it suitable for a wide array of real-world applications:</p>
<ul>
<li><strong>Video Prediction / Next Frame Prediction</strong>: This is a direct application, as shown in our example. ConvLSTM can learn the motion and evolution of objects and patterns within a video, generating realistic future frames. This has implications for:
<ul>
<li><strong>Traffic Flow Prediction</strong>: Anticipating vehicle movement on roads.<br>
</li>
<li><strong>Sports Analysis</strong>: Predicting ball trajectories or player movements.<br>
</li>
</ul></li>
<li><strong>Weather Forecasting and Climate Modeling</strong>: Predicting spatio-temporal patterns of meteorological phenomena like rainfall, temperature maps, or storm movements over time. Radar data, which is essentially a sequence of spatial maps, is a perfect fit.<br>
</li>
<li><strong>Action Recognition in Videos</strong>: Identifying human actions or activities (e.g., walking, running, clapping) by analyzing the sequence of spatial features extracted from video frames.<br>
</li>
<li><strong>Medical Image Analysis (Time Series)</strong>: Analyzing sequences of medical scans (e.g., fMRI for brain activity, dynamic MRI for organ movement, ultrasound videos) to detect changes, track disease progression, or diagnose conditions. Examples include:
<ul>
<li><strong>Tumor Growth Monitoring</strong>: Tracking changes in tumor size and shape over time.</li>
<li><strong>Cardiac Motion Analysis</strong>: Assessing heart wall movement from cine MRI sequences.</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>videojs(video_shortcode_videojs_video1);</script>




</body></html>